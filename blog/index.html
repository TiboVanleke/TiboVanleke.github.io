<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tibo Vanleke | Blog</title>
    <meta name="description" content="Notes and experiments on LLMs, safety, and interpretability.">
    <link rel="icon" href="/favicon.svg" type="image/svg+xml">
    <link rel="icon" href="/favicon.ico" sizes="any">
    <link rel="icon" href="/favicon-32.png" sizes="32x32" type="image/png">
    <link rel="icon" href="/favicon-16.png" sizes="16x16" type="image/png">
    <link rel="apple-touch-icon" href="/favicon-180.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Fraunces:opsz,wght@9..144,600;9..144,700&family=Space+Grotesk:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #f7f3ee;
            --bg-2: #eef6f2;
            --card: #ffffff;
            --text-main: #1a1a1a;
            --text-muted: #5f5f5f;
            --accent: #0b6e4f;
            --line: rgba(0, 0, 0, 0.08);
            --shadow: 0 20px 50px rgba(14, 43, 34, 0.12);
            --radius: 18px;
        }

        body {
            font-family: "Space Grotesk", -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
            background:
                radial-gradient(800px 500px at 10% 0%, var(--bg-2), transparent 60%),
                radial-gradient(700px 500px at 90% 10%, #fef3c7, transparent 55%),
                var(--bg);
            color: var(--text-main);
            margin: 0;
            line-height: 1.6;
            min-height: 100vh;
        }

        .page {
            max-width: 860px;
            margin: 0 auto;
            padding: 48px 20px 80px;
        }

        header {
            background: var(--card);
            border: 1px solid var(--line);
            border-radius: var(--radius);
            box-shadow: var(--shadow);
            padding: 28px;
            margin-bottom: 28px;
        }

        h1 {
            font-family: "Fraunces", serif;
            font-size: clamp(2rem, 3vw, 2.8rem);
            margin: 0 0 6px;
        }

        .muted {
            color: var(--text-muted);
        }

        .post-list {
            display: grid;
            gap: 16px;
        }

        .post-card {
            background: var(--card);
            border: 1px solid var(--line);
            border-radius: 14px;
            padding: 18px;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .post-card:hover {
            transform: translateY(-3px);
            box-shadow: 0 14px 30px rgba(0, 0, 0, 0.1);
        }

        h2 {
            font-family: "Fraunces", serif;
            font-size: 1.5rem;
            margin: 32px 0 12px;
        }

        .post-card a {
            color: var(--text-main);
            text-decoration: none;
            font-weight: 600;
            font-size: 1.1rem;
        }

        .top-link {
            display: inline-flex;
            margin-bottom: 18px;
            color: var(--accent);
            font-weight: 600;
            text-decoration: none;
        }

        footer {
            margin-top: 28px;
            padding-top: 18px;
            border-top: 1px solid var(--line);
            color: var(--text-muted);
            font-size: 0.95rem;
        }
    </style>
</head>
<body>
    <div class="page">
        <a class="top-link" href="/">← Back to home</a>
        <header>
            <h1>Research Notes</h1>
            <div class="muted">Weekly thoughts, paper notes, and experiments around LLM safety and interpretability.</div>
        </header>

        <div class="post-list">
            <article class="post-card">
                <a href="/blog/2026-02-16-hard-attention-turing-completeness-transformers.html">Hard attention and Turing completeness in Transformers</a>
                <div class="muted">February 16, 2026 · 11 min read</div>
                <p class="muted">A deep review of when Transformers are Turing complete, why hard attention matters, and what formal-language limits imply.</p>
            </article>
            <article class="post-card">
                <a href="/blog/2026-02-15-softmax-bottleneck-mixture-of-softmaxes.html">The softmax bottleneck in language modeling</a>
                <div class="muted">February 15, 2026 · 13 min read</div>
                <p class="muted">A deep review of how the softmax bottleneck limits expressivity in language models and how mixture-of-softmaxes raises the effective rank.</p>
            </article>
            <article class="post-card">
                <a href="/blog/2026-02-13-alibi-vs-rope-length-extrapolation.html">ALiBi vs RoPE: positional bias and length extrapolation</a>
                <div class="muted">February 13, 2026 · 11 min read</div>
                <p class="muted">A focused review of why ALiBi extrapolates to longer contexts more reliably than RoPE, and what that implies about positional inductive bias in attention.</p>
            </article>
            <article class="post-card">
                <a href="/blog/2026-02-12-ffn-key-value-memory-factual-recall.html">FFN layers as key-value memories for factual recall</a>
                <div class="muted">February 12, 2026 · 14 min read</div>
                <p class="muted">A deep review of evidence that transformer feed-forward layers behave like key-value memories and localize factual recall in mid-layer MLPs.</p>
            </article>
            <article class="post-card">
                <a href="/blog/2026-02-11-attention-as-modern-hopfield-memory.html">Attention as modern Hopfield memory</a>
                <div class="muted">February 11, 2026 · 13 min read</div>
                <p class="muted">A focused review of the modern Hopfield-network view of attention, with emphasis on storage capacity and retrieval behavior.</p>
            </article>
            <article class="post-card">
                <a href="/blog/2026-02-07-in-context-gradient-descent-linear-regression.html">In-context learning as implicit gradient descent</a>
                <div class="muted">February 7, 2026 · 12 min read</div>
                <p class="muted">A deep dive into gradient-descent-like mechanisms in in-context linear regression.</p>
            </article>
            <article class="post-card">
                <a href="/blog/2026-02-06-grokking-weight-decay-phase-transition.html">Grokking as a phase transition in transformer training</a>
                <div class="muted">February 6, 2026 · 12 min read</div>
                <p class="muted">A deep dive into how weight decay triggers delayed generalization in transformer grokking.</p>
            </article>
            <article class="post-card">
                <a href="/blog/2026-02-05-superposition-sparsity-phase-transition.html">Sparsity as a control knob for superposition</a>
                <div class="muted">February 5, 2026 · 9 min read</div>
                <p class="muted">A deep dive into how sparsity penalties trigger phase transitions between superposition and monosemantic features.</p>
            </article>
            <article class="post-card">
                <a href="/blog/2026-02-04-ioi-circuit-evaluation.html">Evaluating IOI circuits: faithfulness, completeness, minimality</a>
                <div class="muted">February 4, 2026 · 4 min read</div>
                <p class="muted">A focused memo on how to evaluate mechanistic explanations in GPT-2 small.</p>
            </article>
            <article class="post-card">
                <a href="/blog/2026-02-04-bootstrapping-a-weekly-research-memo.html">Bootstrapping a weekly research memo</a>
                <div class="muted">February 4, 2026 · 4 min read</div>
                <p class="muted">A quick pilot for turning weekly reading notes into a public-facing update.</p>
            </article>
        </div>

        <section style="margin-top: 28px;">
            <h2 style="font-size: 1.2rem; margin-bottom: 8px;">Topic log</h2>
            <div class="muted" style="margin-bottom: 12px;">At-a-glance list of covered research questions.</div>
            <ul class="muted" style="margin: 0 0 0 18px; line-height: 1.7;">
                <li>2026-02-16: Hard attention, positional addressing, and Turing completeness in Transformers.</li>
                <li>2026-02-15: The softmax bottleneck and mixture-of-softmaxes as a rank-expanding output layer.</li>
                <li>2026-02-13: ALiBi vs RoPE positional bias and length extrapolation.</li>
                <li>2026-02-12: FFN layers as key-value memories for factual recall.</li>
                <li>2026-02-11: Attention as modern Hopfield memory (capacity and retrieval).</li>
                <li>2026-02-07: In-context learning as implicit gradient descent in linear regression.</li>
                <li>2026-02-06: Grokking phase transitions driven by weight decay in transformer training.</li>
                <li>2026-02-05: Sparsity thresholds and phase transitions in superposition toy models.</li>
                <li>2026-02-04: Evaluating IOI circuit explanations (faithfulness, completeness, minimality).</li>
                <li>2026-02-04: Bootstrapping a weekly research memo format.</li>
            </ul>
        </section>

        <footer>
            Want to collaborate or suggest a topic? Email me at tibo@vanleke.com.
        </footer>
    </div>
</body>
</html>
